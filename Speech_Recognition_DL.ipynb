{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Speech Recognition- DL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFmsTaBTLVgE"
      },
      "source": [
        "# **DISCLAIMER: THE SKELETON OF THE CODE AND VARIOUS FUNCTIONS/CLASSES OF THE CODE ARE BASED UPON EXAMPLE CODE GIVEN IN RECITATION 8 PART 1 AND PART 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZJvyZWrLQwt"
      },
      "source": [
        "# **Importing all Relevant Libraries and Mounting my Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvCr94K8IuHK"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy as sp\n",
        "from matplotlib.pyplot import *\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils import data\n",
        "from torchvision import transforms\n",
        "from torchvision import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, log_loss\n",
        "from functools import partial\n",
        "from dataclasses import dataclass\n",
        "from collections import OrderedDict\n",
        "import os\n",
        "from PIL import Image\n",
        "import torchvision.transforms.functional as TF\n",
        "import time\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import roc_auc_score\n",
        "cuda = torch.cuda.is_available()\n",
        "cuda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3t2iDC_IWyb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "segi7fFSIXea"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5cLfWaHIayw"
      },
      "source": [
        "!kaggle competitions download -c 11-785-fall-20-homework-4-part-2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efYVCxC4IevU"
      },
      "source": [
        "!unzip -q dev.npy.zip\n",
        "!unzip -q test.npy.zip\n",
        "!unzip -q train.npy.zip\n",
        "!unzip -q train_transcripts.npy.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxtxsscgJEMQ"
      },
      "source": [
        "# **Loading the Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OoejK3tdyiM"
      },
      "source": [
        "def load_data():\n",
        "    speech_train = np.load('train.npy', allow_pickle=True, encoding='bytes')\n",
        "    speech_valid = np.load('dev.npy', allow_pickle=True, encoding='bytes')\n",
        "    speech_test = np.load('test.npy', allow_pickle=True, encoding='bytes')\n",
        "\n",
        "    transcript_train = np.load('./train_transcripts.npy', allow_pickle=True,encoding='bytes')\n",
        "    transcript_valid = np.load('./dev_transcripts.npy', allow_pickle=True,encoding='bytes')\n",
        "\n",
        "    return speech_train, speech_valid, speech_test, transcript_train, transcript_valid\n",
        "\n",
        "\n",
        "'''\n",
        "Transforms alphabetical input to numerical input, replace each letter by its corresponding \n",
        "index from letter_list\n",
        "'''\n",
        "def transform_letter_to_index(transcript, letter_list):\n",
        "    idx =[]\n",
        "    l2i= {}\n",
        "    i2l= {}\n",
        "    for id, letter in enumerate(letter_list):\n",
        "      id= l2i[letter]\n",
        "      letter= i2l[id]\n",
        "    for index, label in enumerate(transcript):\n",
        "      for letter in label:\n",
        "        ls= letter.decode('utf-6') \n",
        "        for letter in index:\n",
        "          speech= idx.join(ls)\n",
        "    speech_list. append([i2l[-1]['<eos>']]+ls[letter[:]]+i21[0]['<sos>'])\n",
        "    return speech_list\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Speech2TextDataset(Dataset):\n",
        "    '''\n",
        "    Dataset class for the speech to text data, this may need some tweaking in the\n",
        "    getitem method as your implementation in the collate function may be different from\n",
        "    ours. \n",
        "    '''\n",
        "    def __init__(self, speech, text=None, isTrain=True):\n",
        "        self.speech = speech\n",
        "        self.isTrain = isTrain\n",
        "        if (text is not None):\n",
        "            self.text = text\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.speech.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if (self.isTrain == True):\n",
        "            return torch.tensor(self.speech[index].astype(np.float32)), torch.tensor(self.text[index])\n",
        "        else:\n",
        "            return torch.tensor(self.speech[index].astype(np.float32))\n",
        "\n",
        "\n",
        "def collate_train(batch_data):\n",
        "    i_lens = []\n",
        "    t_lens = []\n",
        "    i_pad = []\n",
        "    t_pad = []\n",
        "   \n",
        "    for idx,b in enumerate(batch_data)):\n",
        "        i_pad= np.append(i_pad, batch_data[idx*-1][-1])\n",
        "        i_padt= torch.tensor(i_pad)\n",
        "        i_lens= np.append(i_lens,batch_data[idx][0] )\n",
        "        i_lenst= torch.tensor(i_lens)\n",
        "        t_pad= np.append(t_pad, batch_data[idx][1][1:len(batch_data)+1])\n",
        "        t_padt= torch.tensor(t_pad)\n",
        "        t_lens= np.append(t_lens, (len(batch_data[idx][0])-len(batch_data[idx][1])))\n",
        "        t_lenst= torch.tensor(t_lens)\n",
        "    inputs_pad = pad_sequence(i_padt) # dim (B, T, C) since batch_first is true, (T, B, C) if false\n",
        "    targets_pad = pad_sequence(t_padt)\n",
        "    return inputs_pad, targets_pad, i_lenst, t_lenst\n",
        "\n",
        "\n",
        "def collate_test(batch_data):\n",
        "    i_pad = []\n",
        "    i_lens = []\n",
        "    for idx,b in enumerate(batch_data)):\n",
        "        i_pad= np.append(i_pad, batch_data[idx*-1][-1])\n",
        "        i_padt= torch.tensor(i_pad)\n",
        "        i_lens= np.append(i_lens,batch_data[idx][0] )\n",
        "        i_lenst= torch.tensor(i_lens)\n",
        "    inputs_pad = pad_sequence(i_padt)\n",
        "    return inputs_pad, i_lenst\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BC5G9IIoJSlS"
      },
      "source": [
        "# **Model Architecture- Based on Recitation 8 part 2 framework**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sueWFQ8Od4gk"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils as utils\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    '''\n",
        "    Attention is calculated using key, value and query from Encoder and decoder.\n",
        "    Below are the set of operations you need to perform for computing attention:\n",
        "        energy = bmm(key, query)\n",
        "        attention = softmax(energy)\n",
        "        context = bmm(attention, value)\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "    def forward(self, query, key, value, lens):\n",
        "  \n",
        "        attention = torch.bmm(value, query.unsqueeze(2)).squeeze(2)\n",
        "        \n",
        "        mask = torch.arange(query.size(1)).unsqueeze(0) >= lens.unsqueeze(1)\n",
        "        \n",
        "        attention.masked_fill_(key, -1e9)\n",
        "        \n",
        "        attention = nn.functional.softmax(attention, dim=1)\n",
        "\n",
        "        out = torch.bmm(attention.unsqueeze(1), value).squeeze(1)\n",
        "\n",
        "        return out, attention\n",
        "\n",
        "\n",
        "class pBLSTM(nn.Module):\n",
        "    '''\n",
        "    Pyramidal BiLSTM\n",
        "    The length of utterance (speech input) can be hundereds to thousands of frames long.\n",
        "    The Paper reports that a direct LSTM implementation as Encoder resulted in slow convergence,\n",
        "    and inferior results even after extensive training.\n",
        "    The major reason is inability of AttendAndSpell operation to extract relevant information\n",
        "    from a large number of input steps.\n",
        "    '''\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(pBLSTM, self).__init__()\n",
        "        self.blstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=1, bidirectional=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        :param x :(N, T) input to the pBLSTM\n",
        "        :return output: (N, T, H) encoded sequence from pyramidal Bi-LSTM \n",
        "        '''\n",
        "        x_lens = np.pad(len(x), batch_first=True)\n",
        "        x_pad = np.pad(x, batch_first=True)\n",
        "        x_lens = x_lens.to(DEVICE)\n",
        "\n",
        "     \n",
        "        x_pad1 = x_pad[(x_pad.size(1) * 2) / 2, :, :] \n",
        "\n",
        "    \n",
        "        x_shape = np.squeeze(x_pad1.size(1) * 1/2*(x_padded.size(2)) * 2, axis=1)\n",
        "        x_lens = x_lens // 2\n",
        "\n",
        "        x_pack = np.concacatnate(x_shape, x_lens)\n",
        "\n",
        "\n",
        "        out, _ = self.blstm(x_pack)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    '''\n",
        "    Encoder takes the utterances as inputs and returns the key and value.\n",
        "    Key and value are nothing but simple projections of the output from pBLSTM network.\n",
        "    '''\n",
        "    def __init__(self, input_dim, hidden_dim, value_size=128,key_size=128):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=1, bidirectional=True)\n",
        "        \n",
        "         self.pBLSTMs = nn.Sequential(\n",
        "            pBLSTM(hidden_dim*4, hidden_dim*3),\n",
        "            pBLSTM(hidden_dim*3, hidden_dim*3),\n",
        "            pBLSTM(hidden_dim*2, hidden_dim)\n",
        "        )\n",
        "\n",
        "        self.key_network = nn.Linear(hidden_dim*2, value_size)\n",
        "        self.value_network = nn.Linear(hidden_dim*2, key_size)\n",
        "\n",
        "    def forward(self, x, lens):\n",
        "        rnn_inp = utils.rnn.pack_padded_sequence(x, lengths=lens, batch_first=False, enforce_sorted=False)\n",
        "        outputs, _ = self.lstm(rnn_inp)\n",
        "\n",
        "        ### Use the outputs and pass it through the pBLSTM blocks! ###\n",
        "        outputs = self.pBLSTMs(outputs)\n",
        "\n",
        "        linear_input, _ = utils.rnn.pad_packed_sequence(outputs)\n",
        "        keys = self.key_network(linear_input)\n",
        "        value = self.value_network(linear_input)\n",
        "\n",
        "        return keys, value\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    '''\n",
        "    As mentioned in a previous recitation, each forward call of decoder deals with just one time step, \n",
        "    thus we use LSTMCell instead of LSLTM here.\n",
        "    The output from the second LSTMCell can be used as query here for attention module.\n",
        "    In place of value that we get from the attention, this can be replace by context we get from the attention.\n",
        "    Methods like Gumble noise and teacher forcing can also be incorporated for improving the performance.\n",
        "    '''\n",
        "    def __init__(self, vocab_size, hidden_dim, value_size=128, key_size=128, isAttended=False):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_dim, padding_idx=0)\n",
        "        self.lstm1 = nn.LSTMCell(input_size=hidden_dim + value_size, hidden_size=hidden_dim)\n",
        "        self.lstm2 = nn.LSTMCell(input_size=hidden_dim, hidden_size=key_size)\n",
        "\n",
        "        self.isAttended = isAttended\n",
        "        if (isAttended == True):\n",
        "            self.attention = Attention()\n",
        "\n",
        "        self.character_prob = nn.Linear(key_size + value_size, vocab_size)\n",
        "\n",
        "    def forward(self, key, values, text=None, isTrain=True, rate=0.01, isGumbel= False):\n",
        "        '''\n",
        "        :param key :(T, N, key_size) Output of the Encoder Key projection layer\n",
        "        :param values: (T, N, value_size) Output of the Encoder Value projection layer\n",
        "        :param text: (N, text_len) Batch input of text with text_length\n",
        "        :param isTrain: Train or eval mode\n",
        "        :return predictions: Returns the character perdiction probability \n",
        "        '''\n",
        "        batch_size = key.shape[1]\n",
        "\n",
        "        if (isTrain == True):\n",
        "            max_len =  text.shape[1]\n",
        "            embeddings = self.embedding(text)\n",
        "        else:\n",
        "            max_len = 250\n",
        "\n",
        "        predictions = []\n",
        "        hidden_states = [None, None]\n",
        "        prediction = torch.zeros(batch_size,1).to(DEVICE)#(torch.ones(batch_size, 1)*33).to(DEVICE)\n",
        "\n",
        "        for i in range(max_len):\n",
        "            # * Implement Gumble noise and teacher forcing techniques \n",
        "            # * When attention is True, replace values[i,:,:] with the context you get from attention.\n",
        "            # * If you haven't implemented attention yet, then you may want to check the index and break \n",
        "            #   out of the loop so you do not get index out of range errors. \n",
        "\n",
        "            if (isTrain):\n",
        "                char_embed = embeddings[:,i,:]\n",
        "                if np.random(0,100) > Rate:\n",
        "                    teacher_forcing = True \n",
        "                else False\n",
        "                 if teacher_forcing is not True:\n",
        "                  \n",
        "                    if i > 0 and isGumbel is True: \n",
        "                        char_embed = self.embedding(np.argmax(prediction[:, axis=1)\n",
        "                    else:\n",
        "                      char_embed0 = torch.nn.functional.gumbel_softmax(prediction)\n",
        "                      chare_embed= char_embed0.mm(self.embedding.weight)\n",
        "                else:\n",
        "                    if i == 0:\n",
        "                        begin = np.zeros(batch_size)\n",
        "                        begin1= np.fill(letter2index['<sos>'])\n",
        "                        begin2=torch.tensor(begin1, dtype=torch.long()) \n",
        "                        char_embed = self.embedding(begin2)\n",
        "                    else:\n",
        "                        char_embed = embeddings[i-1, :, -1]\n",
        "            else:\n",
        "                char_embed = self.embedding(np.argmax(prediction, dim=-1))\n",
        "\n",
        "            inp = torch.cat([char_embed, values[i,:,:]], dim=1)\n",
        "            hidden_states[0] = self.lstm1(inp, hidden_states[0])\n",
        "\n",
        "            inp_2 = hidden_states[0][0]\n",
        "            hidden_states[1] = self.lstm2(inp_2, hidden_states[1])\n",
        "\n",
        "            ### Compute attention from the output of the second LSTM Cell ###\n",
        "            output = hidden_states[1][0]\n",
        "\n",
        "            prediction = self.character_prob(torch.cat([output, values[i,:,:]], dim=1))\n",
        "            predictions.append(prediction.unsqueeze(1))\n",
        "\n",
        "        return torch.cat(predictions, dim=1)\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    '''\n",
        "    We train an end-to-end sequence to sequence model comprising of Encoder and Decoder.\n",
        "    This is simply a wrapper \"model\" for your encoder and decoder.\n",
        "    '''\n",
        "    def __init__(self, input_dim, vocab_size, hidden_dim, value_size=128, key_size=128, isAttended=False):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = Encoder(input_dim, hidden_dim)\n",
        "        self.decoder = Decoder(vocab_size, hidden_dim)\n",
        "\n",
        "    def forward(self, speech_input, speech_len, text_input=None, isTrain=True):\n",
        "        key, value = self.encoder(speech_input, speech_len)\n",
        "        if (isTrain == True):\n",
        "            predictions = self.decoder(key, value, text_input)\n",
        "        else:\n",
        "            predictions = self.decoder(key, value, text=None, isTrain=False)\n",
        "        return predictions\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxksUcfFJe64"
      },
      "source": [
        "# **Train/Test Function -Based on Recitaion 8 part 1 and part 2 framework/code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgY6w546eAD3"
      },
      "source": [
        "import time\n",
        "import torch\n",
        "\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def train(model, train_loader, criterion, optimizer, epoch):\n",
        "    model.train()\n",
        "    model.to(DEVICE)\n",
        "    start = time.time()\n",
        "\n",
        "    torch.manual_seed(11785)\n",
        "encoder = Encoder(len(letters), embed_size=4, hidden_size=4)\n",
        "decoder = Decoder(len(phonemes), embed_size=4, hidden_size=4)\n",
        "# Sum over the batch at every timestep. We manually divide the total loss by number of tokens.(default is 'mean')\n",
        "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
        "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=0.1)\n",
        "\n",
        "loss_history = []\n",
        "\n",
        "for epoch in range(100):\n",
        "    loss = 0\n",
        "    # List of attention vectors\n",
        "    all_attentions = []\n",
        "    \n",
        "    context, state = encoder(X, X_lens)\n",
        "    \n",
        "    context = context.transpose(0, 1)\n",
        "    \n",
        "    state = tuple(st.transpose(0, 1).reshape(X.size(1), -1) for st in state)\n",
        "    \n",
        "    \n",
        "    n_tokens = Y_lens.sum() - Y_lens.size(0)\n",
        "    \n",
        "   \n",
        "    for i in range(Y.size(0) - 1):\n",
        "        out, state, attention = decoder(Y[i], context, X_lens, state)\n",
        "        all_attentions.append(attention.detach())\n",
        "        # Mask of sequenuces that haven't ended (i.e. current tokens are \"real\")\n",
        "        active = i + 1 < Y_lens\n",
        "        # Compute loss only on \"real\" outputs\n",
        "        loss += criterion(out[active], Y[i + 1, active])\n",
        "    \n",
        "  s\n",
        "    loss /= n_tokens\n",
        "    loss_history.append(loss.item())\n",
        "  \n",
        "    all_attentions = torch.stack(all_attentions, dim=1)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print('Final train loss:', loss_history[-1])\n",
        "\n",
        "    end = time.time()\n",
        "\n",
        "def test(model, test_loader, epoch):\n",
        "    model.eval()\n",
        "    test_loss = []\n",
        "    accuracy = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_num, (feats, labels) in enumerate(test_loader):\n",
        "        feats, labels = feats.to(device), labels.to(device)\n",
        "        outputs = model(feats)[1]\n",
        "        \n",
        "        pred_text =[letter2index['<eos>'], letter2index['<pad>']])\n",
        "        target_text = [letter2index['<eos>'], letter2index['<pad>']])\n",
        "        \n",
        "        loss = criterion(outputs, labels.long())\n",
        "        \n",
        "        accuracy += torch.sum(torch.eq(pred_text, target_text)).item()\n",
        "      \n",
        "        del feats\n",
        "        del labels\n",
        "\n",
        "    model.train()\n",
        "    return accuracy, pred_text, target_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N_F7yoQJuky"
      },
      "source": [
        "# **Training and Testing the Netwrok- Based on recitation 8 part 1 and 2 code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oL0Oo9rjf5bm"
      },
      "source": [
        "\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "LETTER_LIST = ['<pad>', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', \\\n",
        "               'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '-', \"'\", '.', '_', '+', ' ','<sos>','<eos>']\n",
        "\n",
        "def main():\n",
        "    model = Seq2Seq(input_dim=40, vocab_size=len(LETTER_LIST), hidden_dim=128)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    nepochs = 25\n",
        "    batch_size = 64 if DEVICE == 'cuda' else 1\n",
        "\n",
        "    speech_train, speech_valid, speech_test, transcript_train, transcript_valid = load_data()\n",
        "    character_text_train = transform_letter_to_index(transcript_train, LETTER_LIST)\n",
        "    character_text_valid = transform_letter_to_index(transcript_valid, LETTER_LIST)\n",
        "\n",
        "    train_dataset = Speech2TextDataset(speech_train, character_text_train)\n",
        "    # val_dataset = \n",
        "    test_dataset = Speech2TextDataset(speech_test, None, False)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_train)\n",
        "    # val_loader = \n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_test)\n",
        "\n",
        "    for epoch in range(nepochs):\n",
        "        train(model, train_loader, criterion, optimizer, epoch)\n",
        "        # val()\n",
        "        test(model, test_loader, epoch)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhZe3S-3K6-b"
      },
      "source": [
        "# **Developing Attention Graphs- Based on recitation 8 part 1 code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tlIKkvCeETo"
      },
      "source": [
        "from matplotlib.lines import Line2D\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "def plot_attn_flow(attn_mask, path):\n",
        "    plt.imsave(path, attn_mask, cmap='hot')\n",
        "    return plt\n",
        "\n",
        "def plot_grad_flow(named_parameters, path):\n",
        "    ave_grads = []\n",
        "    max_grads = []\n",
        "    layers = []\n",
        "    for n, p in named_parameters:\n",
        "        if(p.requires_grad) and (\"bias\" not in n):\n",
        "            if(p is not None):\n",
        "                layers.append(n)\n",
        "                ave_grads.append(p.grad.abs().mean())\n",
        "                max_grads.append(p.grad.abs().max())\n",
        "    plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color=\"c\")\n",
        "    plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color=\"b\")\n",
        "    plt.hlines(0, 0, len(ave_grads)+1, lw=2, color=\"k\" )\n",
        "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
        "    plt.xlim(left=0, right=len(ave_grads))\n",
        "    plt.ylim(bottom = -0.001, top=0.02) # zoom in on the lower gradient regions\n",
        "    plt.xlabel(\"Layers\")\n",
        "    plt.ylabel(\"average gradient\")\n",
        "    plt.title(\"Gradient flow\")\n",
        "    #plt.tight_layout()\n",
        "    plt.grid(True)\n",
        "    plt.legend([Line2D([0], [0], color=\"c\", lw=4),\n",
        "                Line2D([0], [0], color=\"b\", lw=4),\n",
        "                Line2D([0], [0], color=\"k\", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])\n",
        "    plt.show()\n",
        "    plt.savefig(path)\n",
        "    return plt, max_grads\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSRQg_hILro1"
      },
      "source": [
        "# **DISCLAIMER: THE SKELETON OF THE CODE AND VARIOUS FUNCTIONS/CLASSES OF THE CODE ARE BASED UPON EXAMPLE CODE GIVEN IN RECITATION 8 PART 1 AND PART 2**"
      ]
    }
  ]
}